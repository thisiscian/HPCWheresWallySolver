\documentclass[../main.tex]{subfiles}
\begin{document}
  \section{Background Information}
    \subsection{Parallel Programming}
    When implementing parallelism, it is desirable to not only speed up a program, but also to use an appropriate number of cores.
    A useful definition is one of speedup, the amount faster that a parallelised version of a program is to an efficient serial implementation.
    \begin{equation}
      \text{Speedup}(P) := \frac{T_\text{serial}}{T_\text{parallel}(P)}
      \label{speedupdef}
    \end{equation}
    Equation \ref{speedupdef} describes the speedup of a system.
    Here $P$ is the number of computing units (threads, processors etc.), $T_\text{serial}$ is the time the serial program takes and $T_\text{parallel}$ is the parallel time.
    For most systems, the maximum speedup possible is $P$; 2 computing units working on a problem will solve it twice as fast as a single unit. 

    Most implementations do not achieve this, especially for larger numbers of computing units.
    It is worth noting that some programs actually produce super-linear speedup \cite{superlinear}.
    This might happen because multiple processors increase the available cache size beyond the size of the problem
    This means that minimal calls to main memory are required, which are often the major cause of slow down.

    A fixed size program with a speedup that scales directly with the number of units used, is said to have `strong scaling'.
    An example of this is having a program that calculates the sum of each element in a 1024x1024x1024 matrix.
    The number of elements being added is very large, so there is plenty of scope for a massively parallel system.

    Of course, the scaling is not strong for an infinite number of processors.
    For more than 1024x1024x512 processors (a minimum of two elements per processor is needed for addition), there is not enough work to be done per processor.
    This inherently has a negative effect on the speedup.
    The scaling will most likely stop being strong before this, due to Amdahl's law, seen in equation \ref{amdahl}.

    If a program is composed of serial and parallelisable portions, and $\alpha$ is the portion of the program that is purely serial, then the time a parallel program takes is:
    \begin{equation}
      T_{parallel}(P) = T_{serial}\left(\alpha-\frac{1-\alpha}{P}\right)
    \end{equation}
    Inserting this into the previous definition of speedup in equation \ref{speedupdef}, we see Amdahl's Law emerge.
    \begin{equation}
      \text{Speedup}(P) = \frac{T_\text{serial}}{T_\text{parallel}(P)} = \frac{T_\text{serial}}{T_\text{serial}\left(\alpha-frac{1-\alpha}{P}\right)} = \frac{1}{\alpha+\frac{1-\alpha}{P}}
      \label{amdahl}
    \end{equation}
    It is clear that as $P$ tends to large numbers, the speedup approaches the constant value of $1/\alpha$.  
    
    In spite of this, parallel programs regularly achieve `weak scaling'.
    This is the case if a program scales with the number of cores for a fixed amount of work per computing unit.
    Gustafson's law describes the mathematics of weak scaling.
    Here, $P$ is the number of computing units, and $N$ is the size of the problem.
    \begin{align*}
      T(P,N) &= T_\text{serial} + T_{\text{parallel},N} = \alpha + \frac{N(1-\alpha)}{P} \\
      T(1,N) &= T_\text{serial} + NT_{\text{parallel},N} = \alpha + N(1-\alpha) \\
    \end{align*}
    We define $T(P,N)$ as the time for the purely serial components to complete, $T_\text{serial}$, with the time it takes for $P$ processors one task of size $1/N$, $T_{\text{parallel},1/N}$.
    The time for 1 processor to do an equivalent amount of work is defined in $T(1,N)$.
    This is the sum of the serial time $T_\text{serial}$ and every bit of work the processors would do, i.e. $NT_{\text{parallel},N}$.
    \begin{align*}
      \text{Speedup}(P,N) &= \frac{T(1,N)}{T(P,N)} = \frac{\alpha+N(1-\alpha)}{\alpha+\frac{N(1-\alpha)}{P}} 
    \end{align*}
    \begin{equation}
      \text{Speedup}(P,\beta P) = \frac{\alpha+\beta P(1-\alpha)}{\alpha+\beta(1-\alpha)} \propto P
      \label{gustafson}
    \end{equation}
    It is clear that Gustafson's Law produces better scaling, as long as $N$ scales with $P$.
    A trivial example of weak scaling is calculating $\pi$ to 10 digits on each processor, and then adding that to some shared variable.
    The problem size is fixed per processor, but the overall work increases with the number of processors.
    \subsection{Shared Memory Parallelism}
      This project will implement parallelism through the shared memory multiprocessing API known as OpenMP.
  
      %% what is shared memory
      Shared memory multiprocessing is a form of parallelism that relies on the presence of shared memory.
      This is a region of memory that can be accessed equally by several processors.
      Normal examples of this are in multicore systems, which have a shared L2 or L3 cache.
      Having shared memory allows data to be easily shared amongst processors.
      This removes redundant copying of data from main memory, and reduces the overall cache usage.
      Using less of the available cache means that a larger amount of other values can be stored, also reducing calls to main memory.
      In a similar way, messages can also be passed between processors at high speeds.
      Communication through a local cache is considerably faster than typical messaging systems.
      Most recently manufactured computers are multicore, so it is likely that everyday devices will be able to benefit from shared memory parallelism.

      %% openmp
      OpenMP implements shared memory multiprocessing through the use of directives and routines.
      A directive is a compiler flag that informs the compiler that the user intends for a specific behaviour to occur.
      These are simple to include into a program, often requiring little to no changes to a serial program to parallelise it.
      Listing \ref{parhey} shows the simplicity of implementing parallelism.
      \lstset{language=C++}
      \begin{lstlisting}[caption = 'A simple loop parallelised with OpenMP', label=parhey]
        // a simple for loop
        int x[4];
        for(int i=0; i<4; i++) {
          x[i] = i;
        }

        // and again parallelised with OpenMP
        int y[4];
        #pragma omp parallel for default(none) shared(y)
        for(int i=0; i<4; i++) {
          y[i] = i;
        }
      \end{lstlisting}

      %% issues with shared memory multiprocessing
      There are some issues that come with the use of shared memory multiprocessing.
      The main one is that scaling is often poor.
      This is due to the fact that there is a fixed amount of processors attached to any block of shared memory.
      Once the system is scaled beyond this amount, the program starts to become dominated by the speed of message between the two memory systems.
      Futhermore, the speed at which the CPU can write to the memory is limited.
      Increasing the number of processors attempting to write to memory through the same CPU causes a bottleneck to occur.
 
      %%Shared memory as opposed to message passing.
      Another popular form of parallelism is through message passing.
      This is, unsurprisngly, the practice of passing messages between processors.
      In this way, data can be shared between processors, allowing for processors to interact.
      Unlike shared memory parallelism, message passing protocols typically do not rely on shared memory.
      Instead, messages are often sent over the bus, meaning that the processors being used can be in different nodes.

      One of the most commonly implemented standard is the Message Passing Interface (MPI).
      MPI scales to very large numbers of processors.
      However, converting existing code for use in MPI is time consuming, often considerably increasing the maintenance for the code.
      Listing \ref{parMPI} shows one potential way to parallelise the same loop from listing \ref{parhey} using MPI.
      \begin{lstlisting}[caption = 'A simple loop parallelised with MPI', label=parMPI]
        // a simple for loop
        int x[4];
        for(int i=0; i<4; i++) {
          x[i] = i;
        }

        // and again parallelised with MPI
        int y[4];
        int rank;
        MPI_Status status;
        MPI_Comm_rank(&rank);
        if(rank == 0) {
          y[0] = 0;
          MPI_Recv(&y[1], 1, MPI_INT,0,0,MPI_COMM_WORLD, &status);
          MPI_Recv(&y[2], 1, MPI_INT,0,0,MPI_COMM_WORLD, &status);
          MPI_Recv(&y[3], 1, MPI_INT,0,0,MPI_COMM_WORLD, &status);
        } else {
          MPI_Send(rank,1, MPI_INT,0,0, MPI_COMM_WORLD);
        }
      \end{lstlisting}

      MPI requires experienced users to useful, the uninitiated may have difficulty implementing the correct method of parallelism for a given problem.
      Users of a parallel computer vision system may have to produce definitions as they go.
      The complexity of MPI restricts the amount of users who could generate new definitions.
      This runs counter to an everyday use of parallel computer vision; expertise would be required to tailor it every problem it could be applied to.
      
      OpenMP is a good fit for implementing parallelism for everyday parallel computer vision.
      Existing code can be easily modified, and parallelism does not require expertise in parallel techniques.
      The scalability issues associated with shared memory parallelism are unlikely to be prominent in everyday systems.
      %% wrap up better you chump
      
    \subsection{Computer Vision Libraries}
      Computer vision is a sprawling discipline, with decades of research behind it.
      This has led to the development of large libraries that can solve many computer vision problems.
      
      %% opencv
      OpenCV is an open source library used for implementing a wide range of computer vision techniques. 
      It is available in three of the most major operating systems, linux, mac and windows.
      OpenCV is aimed at developing real-time solutions to computer vision problems \cite{learningopencv}.
      %% other libraries

    \subsection{Parallel Computer Vision}
    \subsection{Parallel Image Recognition} %%??
    \subsection{Shape and Colour Analysis}
    One obvious way of analysing an image is to break it down into shapes and colours.
    In linguistic terms, it is easiest to depict an object by describing it's shape and colour, i.e. "the red box" or "the green hand".
    This is conceptually simple to explain and understand, and thus is more intuitive to program.
    Most images are saved as raster images (e.g. PNG, BMP, GIF), which is a 2D array of colours that directly map to set of pixels on the screen.
    This is opposed to vector images that store the location and colour of geometric primitives (squares, circles, triangles, etc.).
    Vector images, rather than raster images, are directly easier to perform shape and colour analysis on.
    However, input devices such as webcams and scanners typically produce images in raster formats.

    Regardless of format, methods of colour analysis are simple to implement programmatically.
    This is because colour is intrinsic to all methods of storing the data of an image.
    Shape analysis is less simple for raster images.
    Shape boundaries must be found, which is normally done through edge detection.
    These boundaries are then analysed to find points of intersection, and the curvature between them.
    The group of curves must then be examined to find what shapes exist.

    These methods allow for a descriptional, heuristic method for locating characters.
    No previous image of Wally is required, only a description, such as "red and white stripes" or "black glasses".
    This allows users to extend the solution past Wally, and towards other characters, who do not have to be seen prior.
    This form of analysis can lead to false positives, and should be combined with other results.

    An example of the usefulness of this, beyond Where's Wally, could be found in augmented reality technology, such as Google Glass.
    A common problem people experience is losing their keys.
    Users with access to AR devices could use colour and shape analysis to enhance their searching (i.e. if they are visible, but in a cluttered area).
    As keys generally have a few well defined shapes and colour schemes, the device would not have to store what the keys look like in advance.
    Assuming that parallelism is available, this could potentially be done faster than the human eye can search, helping the user significantly.
    
    \subsection{Feature Analysis}
    Some of the most reliable computer vision algorithms (such as SIFT \cite{sift}), were developed while considering the neuroscience of human vision.
    Tanaka\cite{mechobjrecog} and Perrett and Oram\cite{perretthv} found that human object recognition identifies objects with features that are invariant to brightness, scale and position.
    These results have been used as inspiration for feature analysis.
    This technique finds features; regions of an image which are scale, rotation and illumination invariant.
    These features are most immediately useful when compared with the features of another image.
    For example, the features from an image of just Wally can be used to locate Wally in a normal puzzle image.

    This method generally requires an existing image to find Wally, which restricts the flexibility of the search.
    However, this method is very reliable, as long as Wally is not obscured, it will likely locate him correctly.
    Normally, Wally is obscured, so this method should be combined with other techniques.

    Feature analysis benefits from task farming when there is an image needs to be searched for a large number of sub-images.
    A beyond Wally example would be in detecting employees going into work on a flexitime basis.
    This would use photos of each employee combined with CCTV to note the time that workers enter and leave their workplace.
    Users would not need any form of ID other than their own faces, and this can be combined with existing security systems.    
    \biblio
\end{document}

