\documentclass[../main.tex]{subfiles}
\begin{document}
  \section{Literature Review}
    \subsection{Feature Recognition}
      Detecting features within an image is an important technique that should be used in object recognition.
      One of the most robust algorithms available for this is SIFT (Scale-Invariant Feature Transform), developed by David Lowe\cite{sift}.
      This algorithm uses various techniques to find scale, rotationally and translationally invariant keys, which are also partially invariant under affine transforms and changes in illumination.
      The keys contain feature vectors, which describe the area around them.
      This algorithm is very reliable at finding a given subimage within a larger image.
      However, some of the techniques required to produce reliable results require unexpected amounts of memory.
      To create a scale-space version of the image, four versions of the image must be produced, one of which is scaled to be twice the size of the original.
      This is not a problem when analysing a single image, or analysing multiple images.
      If the algorithm is used for analysing multiple images concurrently, memory constraints would limit it's efficiency.
      This can be seen with Zhang's parallel implementation of SIFT \cite{zhangsift}, where the scale-space creation is one of the areas the program spends most time on.
      This paper shows resonable parallel efficiency, but only has results up to 32 cores.
      The paper also only solves 5 images at a time, which gives a 7 times speedup over an optimised version of SIFT.
      This numer of images is not large enough to show the true limits of shared memory parallelism.

      SURF (Speeded-Up Robust Features), developed by Bay et. al.\cite{surf}, offers a similarly robust solution, but with greater efficiency.
    \biblio
\end{document}
